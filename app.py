import streamlit as st
import streamlit.components.v1 as components
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup, NavigableString, Tag, Comment
import time
import subprocess
import os
import re
from io import BytesIO

# Wordä½œæˆç”¨
from docx import Document
from docx.shared import RGBColor, Pt
from docx.enum.text import WD_ALIGN_PARAGRAPH

# ==========================================
# è¨­å®š
# ==========================================
FIXED_ENTRY_URL = "https://www.h-ken.net/mypage/20250611_1605697556/"

# ==========================================
# ã‚µãƒ¼ãƒãƒ¼è¨­å®š
# ==========================================
def install_playwright():
    try:
        subprocess.run(["playwright", "install", "chromium"], check=True)
    except Exception as e:
        print(f"Install error: {e}")

if "setup_done" not in st.session_state:
    with st.spinner("ã‚µãƒ¼ãƒãƒ¼èµ·å‹•ä¸­..."):
        install_playwright()
        st.session_state.setup_done = True

# ==========================================
# è‰²è§£æãƒ­ã‚¸ãƒƒã‚¯
# ==========================================
def get_rgb_from_str(color_str):
    if not color_str: return None
    c = color_str.lower().strip()
    
    # Hex
    hex_match = re.search(r'#([0-9a-f]{6})', c)
    if hex_match:
        h = hex_match.group(1)
        return RGBColor(int(h[:2], 16), int(h[2:4], 16), int(h[4:], 16))
    
    # åŸºæœ¬è‰²ãƒãƒƒãƒ—
    colors = {
        'red': RGBColor(255, 0, 0),
        'blue': RGBColor(0, 0, 255),
        'green': RGBColor(0, 128, 0),
        'lightseagreen': RGBColor(32, 178, 170),
        'pink': RGBColor(255, 192, 203),
        'orange': RGBColor(255, 165, 0),
        'purple': RGBColor(128, 0, 128),
        'gray': RGBColor(128, 128, 128),
        'black': RGBColor(0, 0, 0)
    }
    return colors.get(c.split()[0])

def parse_css_colors(soup):
    css_map = {}
    for style in soup.find_all('style'):
        if style.string:
            matches = re.finditer(r'\.([a-zA-Z0-9_-]+)\s*\{[^}]*color\s*:\s*([^;\}]+)', style.string, re.IGNORECASE)
            for m in matches:
                class_name = m.group(1)
                color_val = m.group(2).strip()
                rgb = get_rgb_from_str(color_val)
                if rgb:
                    css_map[class_name] = rgb
    css_map.update({
        'conversation': RGBColor(255, 0, 0),
        'marker': RGBColor(255, 0, 0)
    })
    return css_map

def apply_style_to_run(run, element, css_map):
    style_attr = element.get('style', '').lower()
    classes = element.get('class', [])
    
    # å¤ªå­—
    if element.name in ['b', 'strong', 'h1', 'h2'] or 'font-weight:bold' in style_attr or 'bold' in classes:
        run.bold = True
        
    # è‰²
    rgb = None
    if 'color' in style_attr:
        m = re.search(r'color\s*:\s*([^;"]+)', style_attr)
        if m: rgb = get_rgb_from_str(m.group(1))
    
    if not rgb and classes:
        for cls in classes:
            if cls in css_map:
                rgb = css_map[cls]
                break
                
    if not rgb and element.get('color'):
        rgb = get_rgb_from_str(element.get('color'))

    if rgb:
        run.font.color.rgb = rgb

# ==========================================
# Wordä½œæˆã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆæ”¹è¡Œå¯¾å¿œç‰ˆï¼‰
# ==========================================
# æ”¹è¡Œã‚’å…¥ã‚Œã‚‹ã¹ããƒ–ãƒ­ãƒƒã‚¯è¦ç´ ã®ãƒªã‚¹ãƒˆ
BLOCK_TAGS = ['p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'blockquote', 'li', 'article', 'section', 'header', 'footer']

def process_node_recursive(paragraph, node, css_map):
    """å†å¸°çš„ã«ãƒãƒ¼ãƒ‰ã‚’å‡¦ç†ã—ã¦Wordã«è¿½åŠ """
    if isinstance(node, NavigableString):
        text = str(node)
        # æœ¬æ–‡ä»¥å¤–ã®ã‚·ã‚¹ãƒ†ãƒ ã‚³ãƒ¡ãƒ³ãƒˆã‚’é™¤å¤–
        if "contents_within" not in text and text.strip():
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
            run = paragraph.add_run(text)
            if node.parent:
                apply_style_to_run(run, node.parent, css_map)
                
    elif isinstance(node, Tag):
        # 1. æ”¹è¡Œã‚¿ã‚°ã®å ´åˆ
        if node.name == 'br':
            paragraph.add_run('\n')
            
        # 2. ç„¡è¦–ã™ã‚‹ã‚¿ã‚°
        elif node.name in ['script', 'style', 'noscript']:
            pass
            
        # 3. ãã®ä»–ã®ã‚¿ã‚°
        else:
            # ãƒ–ãƒ­ãƒƒã‚¯è¦ç´ ã®å ´åˆã€å‡¦ç†ã®å‰å¾Œã«æ”¹è¡Œã®æ¦‚å¿µãŒã‚ã‚‹ãŒã€
            # å†å¸°å‡¦ç†å†…ã§ã¯ã€Œä¸­èº«ã‚’å‡¦ç†ã—ãŸå¾Œã«æ”¹è¡Œã‚’è¿½åŠ ã€ã™ã‚‹ã®ãŒå®‰å…¨
            
            # å­è¦ç´ ã‚’å†å¸°å‡¦ç†
            for child in node.children:
                process_node_recursive(paragraph, child, css_map)
            
            # ã€é‡è¦ã€‘ãƒ–ãƒ­ãƒƒã‚¯è¦ç´ ãŒçµ‚ã‚ã£ãŸã‚‰æ”¹è¡Œã‚’å…¥ã‚Œã‚‹
            # ãŸã ã—ã€æœ€å¾Œã®è¦ç´ ã§ãªã‘ã‚Œã°
            if node.name in BLOCK_TAGS:
                paragraph.add_run('\n')

def create_rich_docx(title_html, body_html, css_map):
    doc = Document()
    
    # --- ã‚¿ã‚¤ãƒˆãƒ« ---
    soup_title = BeautifulSoup(title_html, 'html.parser')
    p_title = doc.add_paragraph()
    p_title.alignment = WD_ALIGN_PARAGRAPH.LEFT
    process_node_recursive(p_title, soup_title, css_map)
    
    for run in p_title.runs:
        run.font.size = Pt(16)
        if not run.bold: run.bold = True

    doc.add_paragraph("") # ç©ºè¡Œ

    # --- æœ¬æ–‡ ---
    soup_body = BeautifulSoup(body_html, 'html.parser')
    
    # ä»¥å‰ã®ã‚ˆã†ã«find_allã§ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆ†ã‘ã‚‹ã¨å…¥ã‚Œå­ãŒå´©ã‚Œã‚‹ãŸã‚ã€
    # å…¨ä½“ã‚’1ã¤ã®å¤§ããªæ®µè½ã¨ã—ã¦å‡¦ç†ã—ã¤ã¤ã€å†…éƒ¨ã§ '\n' ã‚’æŒŸã‚€æˆ¦ç•¥ã‚’ã¨ã‚‹
    # ã¾ãŸã¯ã€ãƒ«ãƒ¼ãƒˆç›´ä¸‹ã®ãƒ–ãƒ­ãƒƒã‚¯ã”ã¨ã«æ®µè½ã‚’åˆ†ã‘ã‚‹
    
    # ãƒ«ãƒ¼ãƒˆç›´ä¸‹ã®è¦ç´ ã‚’å–å¾—
    top_level_elements = soup_body.find_all(True, recursive=False)
    
    if not top_level_elements:
        # ãƒ«ãƒ¼ãƒˆç›´ä¸‹ã«ãƒ†ã‚­ã‚¹ãƒˆã—ã‹ãªã„å ´åˆ
        p = doc.add_paragraph()
        process_node_recursive(p, soup_body, css_map)
    else:
        for element in top_level_elements:
            # æ–°ã—ã„æ®µè½ã‚’ä½œæˆ
            p = doc.add_paragraph()
            # ãã®è¦ç´ ã®ä¸­èº«ã‚’å†å¸°çš„ã«è¿½åŠ ï¼ˆå†…éƒ¨ã®æ”¹è¡Œã¯ '\n' ã«ãªã‚‹ï¼‰
            process_node_recursive(p, element, css_map)
            
            # æ®µè½é–“ã®ä½™ç™½èª¿æ•´ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            # p.paragraph_format.space_after = Pt(6)
    
    buffer = BytesIO()
    doc.save(buffer)
    buffer.seek(0)
    return buffer

# ==========================================
# ãƒ–ãƒ©ã‚¦ã‚¶æ“ä½œ
# ==========================================
def fetch_html_force_clean(target_url):
    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=True,
            args=['--no-sandbox', '--disable-dev-shm-usage', '--disable-gpu']
        )
        iphone_12 = p.devices['iPhone 12']
        context = browser.new_context(**iphone_12)
        page = context.new_page()

        try:
            page.goto(FIXED_ENTRY_URL, timeout=30000)
            time.sleep(2) 
            page.goto(target_url, timeout=30000)
            page.wait_for_load_state("domcontentloaded")
            time.sleep(2) 

            # ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ç ´å£ŠJS
            page.evaluate("""
                () => {
                    const keywords = ['ã¯ã„', 'YES', 'Yes', '18æ­³', 'Enter', 'å…¥ã‚Šå£', 'å…¥å ´'];
                    const buttons = document.querySelectorAll('a, button, div, span');
                    for (let btn of buttons) {
                        if (keywords.some(k => btn.innerText && btn.innerText.includes(k))) {
                            btn.click();
                        }
                    }
                    const allDivs = document.querySelectorAll('body > div, body > section');
                    allDivs.forEach(div => {
                        const style = window.getComputedStyle(div);
                        if (style.position === 'fixed' && style.zIndex > 50) {
                            div.remove();
                        }
                    });
                    document.body.style.overflow = 'visible';
                    document.body.style.height = 'auto';
                }
            """)
            time.sleep(1) 
            return page.content()
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
            return None
        finally:
            browser.close()

# ==========================================
# æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ï¼ˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¼·åŒ–ï¼‰
# ==========================================
def extract_target_content(html_content, target_url):
    soup = BeautifulSoup(html_content, 'html.parser')

    # CSSè§£æ
    css_map = parse_css_colors(soup)
    
    # è¡¨ç¤ºç”¨ã‚¹ã‚¿ã‚¤ãƒ«
    styles = []
    for link in soup.find_all('link', rel='stylesheet'):
        styles.append(str(link))
    for style in soup.find_all('style'):
        styles.append(str(style))
    style_html = "\n".join(styles)

    # ã‚¿ã‚¤ãƒˆãƒ«
    title_html = ""
    target_h1 = soup.find("h1", class_="pageTitle")
    if target_h1:
        title_html = str(target_h1)
    else:
        target_h1 = soup.find("h1")
        if target_h1:
            title_html = str(target_h1)

    # æœ¬æ–‡
    body_html = "<div>æœ¬æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ</div>"
    
    target_div = soup.find(id="sentenceBox")
    if not target_div:
        target_div = soup.find(id="main_txt")

    if target_div:
        # ã‚³ãƒ¡ãƒ³ãƒˆå‰Šé™¤
        for comment in target_div.find_all(string=lambda text: isinstance(text, Comment)):
            comment.extract()
            
        # ä¸è¦ã‚¿ã‚°å‰Šé™¤
        for bad in target_div.find_all(["script", "noscript", "iframe", "form", "button", "input"]):
            bad.decompose()

        # æ–‡æœ«ã‚«ãƒƒãƒˆ
        cut_point = target_div.find(class_="kakomiPop2")
        if cut_point:
            for sibling in cut_point.find_next_siblings():
                sibling.decompose()
            cut_point.decompose()

        # è­¦å‘Šæ–‡å‰Šé™¤ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹åˆ¤å®šï¼‰
        bad_words = ["ç„¡æ–­è»¢è¼‰", "Googleã«é€šå ±", "åˆ‘äº‹å‘Šè¨´", "æ°‘äº‹è¨´è¨Ÿ", "ã‚¨ãƒã‚±ãƒ³", "contents_within"]
        for tag in target_div.find_all(['p', 'div', 'span', 'font', 'b']):
            text = tag.get_text()
            if any(w in text for w in bad_words):
                if len(text) < 400:
                    tag.decompose()

        body_html = str(target_div)

    # HTMLãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    final_html = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="utf-8">
        <base href="{target_url}">
        {style_html}
        <style>
            body {{
                background-color: #fff;
                padding: 15px;
                font-family: sans-serif;
                overflow: auto !important;
            }}
            h1.pageTitle {{
                font-size: 20px;
                margin-bottom: 20px;
                border-bottom: 1px solid #ccc;
                padding-bottom: 10px;
                line-height: 1.4;
            }}
            #sentenceBox {{
                font-size: 16px;
                line-height: 1.8;
                color: #333;
            }}
        </style>
    </head>
    <body>
        {title_html}
        {body_html}
    </body>
    </html>
    """

    return title_html, body_html, final_html, css_map

# ==========================================
# ç”»é¢æ§‹æˆ
# ==========================================
st.set_page_config(page_title="H-Review Final", layout="centered")

st.title("ğŸ’ å®Œæˆç‰ˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡º")
st.caption("è­¦å‘Šæ–‡å‰Šé™¤ãƒ»æ”¹è¡Œå¯¾å¿œãƒ»è‰²ä»˜ãWordä¿å­˜")

url = st.text_input("èª­ã¿ãŸã„è¨˜äº‹ã®URL", placeholder="https://...")

if st.button("æŠ½å‡ºã‚’é–‹å§‹ã™ã‚‹", type="primary", use_container_width=True):
    if not url:
        st.warning("URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        status = st.empty()
        status.info("â³ è§£æä¸­...")
        
        html = fetch_html_force_clean(url)

        if html:
            status.info("ğŸ“„ ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")
            
            title_html_str, body_html_str, final_html_preview, css_map = extract_target_content(html, url)
            
            status.empty()
            st.success("å®Œäº†ï¼")
            
            docx_file = create_rich_docx(title_html_str, body_html_str, css_map)
            
            st.download_button(
                label="ğŸ“˜ Word(.docx) ã§ä¿å­˜",
                data=docx_file,
                file_name="story_colored.docx",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                use_container_width=True
            )
            
            st.divider()
            components.html(final_html_preview, height=800, scrolling=True)
            
        else:
            status.error("èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
