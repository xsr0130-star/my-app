import streamlit as st
import streamlit.components.v1 as components
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import time
import subprocess
import os
import requests
from io import BytesIO

# Word/PDFä½œæˆç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from docx import Document
from reportlab.pdfgen import canvas
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont
from reportlab.lib.pagesizes import A4
from reportlab.lib.units import mm
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.enums import TA_JUSTIFY

# ==========================================
# è¨­å®šï¼šå…¥ã‚Šå£URL
# ==========================================
FIXED_ENTRY_URL = "https://www.h-ken.net/mypage/20250611_1605697556/"

# ==========================================
# ã‚µãƒ¼ãƒãƒ¼è¨­å®š
# ==========================================
def install_playwright():
    try:
        subprocess.run(["playwright", "install", "chromium"], check=True)
    except Exception as e:
        print(f"Install error: {e}")

if "setup_done" not in st.session_state:
    with st.spinner("ã‚µãƒ¼ãƒãƒ¼èµ·å‹•ä¸­..."):
        install_playwright()
        st.session_state.setup_done = True

# ==========================================
# ä¾¿åˆ©é–¢æ•°ï¼šæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®ç¢ºä¿ï¼ˆPDFç”¨ï¼‰
# ==========================================
def ensure_japanese_font():
    """PDFä½œæˆç”¨ã«IPAexã‚´ã‚·ãƒƒã‚¯ãƒ•ã‚©ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
    font_path = "IPAexGothic.ttf"
    if not os.path.exists(font_path):
        # å®‰å®šã—ãŸIPAãƒ•ã‚©ãƒ³ãƒˆã®é…å¸ƒå…ˆï¼ˆGitHubç­‰ã®ãƒŸãƒ©ãƒ¼ï¼‰ã‹ã‚‰å–å¾—
        url = "https://github.com/minoryorg/ipaex-font/raw/refs/heads/master/ipaexg.ttf"
        try:
            response = requests.get(url)
            with open(font_path, "wb") as f:
                f.write(response.content)
        except:
            pass
    return font_path

# ==========================================
# Wordãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆé–¢æ•°
# ==========================================
def create_docx(title, clean_text_list):
    doc = Document()
    doc.add_heading(title, 0)
    
    for text in clean_text_list:
        if text.strip():
            doc.add_paragraph(text)
            
    buffer = BytesIO()
    doc.save(buffer)
    buffer.seek(0)
    return buffer

# ==========================================
# PDFãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆé–¢æ•°ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ï¼‰
# ==========================================
def create_pdf(title, clean_text_list):
    buffer = BytesIO()
    doc = SimpleDocTemplate(buffer, pagesize=A4,
                            rightMargin=20*mm, leftMargin=20*mm,
                            topMargin=20*mm, bottomMargin=20*mm)
    
    # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆç™»éŒ²
    font_path = ensure_japanese_font()
    if os.path.exists(font_path):
        pdfmetrics.registerFont(TTFont('Japanese', font_path))
        font_name = 'Japanese'
    else:
        font_name = 'Helvetica' # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆæ–‡å­—åŒ–ã‘ã™ã‚‹å¯èƒ½æ€§ã‚ã‚Šï¼‰

    styles = getSampleStyleSheet()
    
    # æ—¥æœ¬èªç”¨ã‚¹ã‚¿ã‚¤ãƒ«å®šç¾©
    style_body = ParagraphStyle(name='JapaneseBody',
                                parent=styles['Normal'],
                                fontName=font_name,
                                fontSize=10.5,
                                leading=16, # è¡Œé–“
                                spaceAfter=6,
                                alignment=TA_JUSTIFY)
                                
    style_title = ParagraphStyle(name='JapaneseTitle',
                                 parent=styles['Heading1'],
                                 fontName=font_name,
                                 fontSize=16,
                                 leading=20,
                                 spaceAfter=20)

    story = []
    
    # ã‚¿ã‚¤ãƒˆãƒ«è¿½åŠ 
    story.append(Paragraph(title, style_title))
    
    # æœ¬æ–‡è¿½åŠ 
    for text in clean_text_list:
        if text.strip():
            # PDFç”Ÿæˆæ™‚ã«ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ç‰¹æ®Šæ–‡å­—ã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
            safe_text = text.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')
            story.append(Paragraph(safe_text, style_body))
            story.append(Spacer(1, 2*mm))

    doc.build(story)
    buffer.seek(0)
    return buffer

# ==========================================
# ãƒ–ãƒ©ã‚¦ã‚¶æ“ä½œ
# ==========================================
def fetch_html_force_clean(target_url):
    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=True,
            args=['--no-sandbox', '--disable-dev-shm-usage', '--disable-gpu']
        )
        iphone_12 = p.devices['iPhone 12']
        context = browser.new_context(**iphone_12)
        page = context.new_page()

        try:
            page.goto(FIXED_ENTRY_URL, timeout=30000)
            time.sleep(2) 
            page.goto(target_url, timeout=30000)
            page.wait_for_load_state("domcontentloaded")
            time.sleep(2) 

            # ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ç ´å£Š
            page.evaluate("""
                () => {
                    const keywords = ['ã¯ã„', 'YES', 'Yes', '18æ­³', 'Enter', 'å…¥ã‚Šå£', 'å…¥å ´'];
                    const buttons = document.querySelectorAll('a, button, div, span');
                    for (let btn of buttons) {
                        if (keywords.some(k => btn.innerText && btn.innerText.includes(k))) {
                            btn.click();
                        }
                    }
                    const allDivs = document.querySelectorAll('body > div, body > section');
                    allDivs.forEach(div => {
                        const style = window.getComputedStyle(div);
                        if (style.position === 'fixed' && style.zIndex > 50) {
                            div.remove();
                        }
                    });
                    document.body.style.overflow = 'visible';
                    document.body.style.height = 'auto';
                }
            """)
            time.sleep(1) 
            return page.content()
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
            return None
        finally:
            browser.close()

# ==========================================
# æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ï¼ˆHTMLè¡¨ç¤ºç”¨ ï¼† ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ç”¨ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼‰
# ==========================================
def extract_target_content(html_content, target_url):
    soup = BeautifulSoup(html_content, 'html.parser')

    # CSSç¢ºä¿
    styles = []
    for link in soup.find_all('link', rel='stylesheet'):
        styles.append(str(link))
    for style in soup.find_all('style'):
        styles.append(str(style))
    style_html = "\n".join(styles)

    # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
    title_html = ""
    title_text_clean = "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"
    target_h1 = soup.find("h1", class_="pageTitle")
    
    if target_h1:
        title_html = str(target_h1)
        title_text_clean = target_h1.get_text(strip=True)
    else:
        target_h1 = soup.find("h1")
        if target_h1:
            title_html = str(target_h1)
            title_text_clean = target_h1.get_text(strip=True)

    simple_title_text = soup.title.get_text(strip=True) if soup.title else "æŠ½å‡ºçµæœ"

    # æœ¬æ–‡æŠ½å‡º
    body_html = "<div>æœ¬æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ</div>"
    text_list_for_file = [] # Word/PDFä¿å­˜ç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆãƒªã‚¹ãƒˆ
    
    target_div = soup.find(id="sentenceBox")
    if not target_div:
        target_div = soup.find(id="main_txt")

    if target_div:
        # ã‚´ãƒŸæƒé™¤
        for bad in target_div.find_all(["script", "noscript", "iframe", "form", "button", "input"]):
            bad.decompose()

        # ä¸è¦ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆkakomiPop2ä»¥é™ï¼‰ã®ã‚«ãƒƒãƒˆ
        cut_point = target_div.find(class_="kakomiPop2")
        if cut_point:
            for sibling in cut_point.find_next_siblings():
                sibling.decompose()
            cut_point.decompose()

        # HTMLä¿å­˜
        body_html = str(target_div)
        
        # Word/PDFç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆæ”¹è¡Œã‚’æ„è­˜ï¼‰
        # pã‚¿ã‚°ã‚„divã‚¿ã‚°ã”ã¨ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        for elem in target_div.find_all(['p', 'div', 'h2', 'h3', 'br']):
            txt = elem.get_text(strip=True)
            if txt:
                text_list_for_file.append(txt)

    # è¡¨ç¤ºç”¨HTML
    final_html = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="utf-8">
        <base href="{target_url}">
        {style_html}
        <style>
            body {{
                background-color: #fff;
                padding: 15px;
                font-family: sans-serif;
                overflow: auto !important;
            }}
            h1.pageTitle {{
                font-size: 20px;
                margin-bottom: 20px;
                border-bottom: 1px solid #ccc;
                padding-bottom: 10px;
                line-height: 1.4;
            }}
            #sentenceBox {{
                font-size: 16px;
                line-height: 1.8;
                color: #333;
            }}
        </style>
    </head>
    <body>
        {title_html}
        {body_html}
    </body>
    </html>
    """

    return simple_title_text, title_text_clean, text_list_for_file, final_html

# ==========================================
# ç”»é¢æ§‹æˆ
# ==========================================
st.set_page_config(page_title="H-Review Pro", layout="wide") # ç”»é¢ã‚’åºƒãä½¿ã†

st.title("ğŸ’ å®Œå…¨ç‰ˆãƒªãƒ¼ãƒ€ãƒ¼ (ä¿å­˜æ©Ÿèƒ½ä»˜ã)")
st.caption("æŠ½å‡ºãƒ»è¡¨ç¤ºãƒ»Word/PDFä¿å­˜ãŒå¯èƒ½ã§ã™ã€‚")

# ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼šå·¦ã«å…¥åŠ›ã€å³ã«ãƒœã‚¿ãƒ³
col1, col2 = st.columns([3, 1])

with col1:
    url = st.text_input("èª­ã¿ãŸã„è¨˜äº‹ã®URL", placeholder="https://...")

if st.button("æŠ½å‡ºã™ã‚‹", type="primary"):
    if not url:
        st.warning("URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        status = st.empty()
        status.text("èª­ã¿è¾¼ã¿ä¸­...")
        
        html = fetch_html_force_clean(url)

        if html:
            status.text("ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")
            
            # æŠ½å‡ºå‡¦ç†
            # è¿”ã‚Šå€¤ãŒå¢—ãˆã¾ã—ãŸ: (ã‚¿ãƒ–ã‚¿ã‚¤ãƒˆãƒ«, è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«, æœ¬æ–‡ãƒªã‚¹ãƒˆ, è¡¨ç¤ºç”¨HTML)
            page_title, article_title, text_list, final_html = extract_target_content(html, url)
            
            status.empty()
            st.success("å®Œäº†")
            
            # --- ä¿å­˜ãƒœã‚¿ãƒ³ã‚¨ãƒªã‚¢ (ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«è¨­ç½®) ---
            st.sidebar.markdown("### ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
            
            # 1. Wordãƒœã‚¿ãƒ³
            docx_file = create_docx(article_title, text_list)
            st.sidebar.download_button(
                label="ğŸ“„ Word (.docx) ã§ä¿å­˜",
                data=docx_file,
                file_name="story.docx",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
            )
            
            # 2. PDFãƒœã‚¿ãƒ³
            pdf_file = create_pdf(article_title, text_list)
            st.sidebar.download_button(
                label="ğŸ“• PDF (.pdf) ã§ä¿å­˜",
                data=pdf_file,
                file_name="story.pdf",
                mime="application/pdf"
            )

            # --- ãƒ¡ã‚¤ãƒ³ç”»é¢è¡¨ç¤º ---
            components.html(final_html, height=800, scrolling=True)
            
        else:
            status.error("èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
