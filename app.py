import streamlit as st
import streamlit.components.v1 as components
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import time
import subprocess
import os
import requests
from io import BytesIO

# Word/PDFä½œæˆç”¨
from docx import Document
from reportlab.pdfgen import canvas
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont, TTFError
from reportlab.lib.pagesizes import A4
from reportlab.lib.units import mm
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.enums import TA_JUSTIFY

# ==========================================
# è¨­å®šï¼šå…¥ã‚Šå£URL
# ==========================================
FIXED_ENTRY_URL = "https://www.h-ken.net/mypage/20250611_1605697556/"

# ==========================================
# ã‚µãƒ¼ãƒãƒ¼è¨­å®š
# ==========================================
def install_playwright():
    try:
        subprocess.run(["playwright", "install", "chromium"], check=True)
    except Exception as e:
        print(f"Install error: {e}")

if "setup_done" not in st.session_state:
    with st.spinner("ã‚µãƒ¼ãƒãƒ¼èµ·å‹•ä¸­..."):
        install_playwright()
        st.session_state.setup_done = True

# ==========================================
# ã€ä¿®æ­£ã€‘æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆç¢ºä¿ï¼ˆè‡ªå·±ä¿®å¾©æ©Ÿèƒ½ä»˜ãï¼‰
# ==========================================
def ensure_japanese_font():
    font_filename = "IPAexGothic.ttf"
    # ã‚ˆã‚Šå®‰å®šã—ãŸURLã«å¤‰æ›´ï¼ˆGitHubã®Rawãƒ‡ãƒ¼ã‚¿ï¼‰
    font_url = "https://raw.githubusercontent.com/minoryorg/ipaex-font/master/ipaexg.ttf"
    
    # 1. å£Šã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒæ®‹ã£ã¦ã„ãŸã‚‰å‰Šé™¤ã™ã‚‹ãƒã‚§ãƒƒã‚¯
    if os.path.exists(font_filename):
        # ã‚µã‚¤ã‚ºãŒå°ã•ã™ãã‚‹(1MBä»¥ä¸‹)å ´åˆã¯ã€å¤±æ•—ã—ãŸã‚´ãƒŸãƒ•ã‚¡ã‚¤ãƒ«ã¨ã¿ãªã—ã¦æ¶ˆã™
        if os.path.getsize(font_filename) < 1000 * 1000:
            os.remove(font_filename)

    # 2. ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã‘ã‚Œã°ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    if not os.path.exists(font_filename):
        try:
            response = requests.get(font_url, timeout=30)
            if response.status_code == 200:
                with open(font_filename, "wb") as f:
                    f.write(response.content)
            else:
                return None
        except Exception:
            return None
            
    return font_filename if os.path.exists(font_filename) else None

# ==========================================
# Wordãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
# ==========================================
def create_docx(title, clean_text_list):
    doc = Document()
    doc.add_heading(title, 0)
    
    for text in clean_text_list:
        if text.strip():
            doc.add_paragraph(text)
            
    buffer = BytesIO()
    doc.save(buffer)
    buffer.seek(0)
    return buffer

# ==========================================
# PDFãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
# ==========================================
def create_pdf(title, clean_text_list):
    buffer = BytesIO()
    doc = SimpleDocTemplate(buffer, pagesize=A4,
                            rightMargin=20*mm, leftMargin=20*mm,
                            topMargin=20*mm, bottomMargin=20*mm)
    
    # ãƒ•ã‚©ãƒ³ãƒˆèª­ã¿è¾¼ã¿ï¼ˆå¤±æ•—æ™‚ã¯è‹±èªãƒ•ã‚©ãƒ³ãƒˆã¸é€ƒã’ã‚‹ï¼‰
    font_path = ensure_japanese_font()
    font_name = 'Helvetica' # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    
    if font_path:
        try:
            pdfmetrics.registerFont(TTFont('Japanese', font_path))
            font_name = 'Japanese'
        except TTFError:
            # ä¸‡ãŒä¸€ãƒ•ã‚©ãƒ³ãƒˆãŒå£Šã‚Œã¦ã„ãŸã‚‰å‰Šé™¤ã—ã¦æ¬¡å›ã«å‚™ãˆã‚‹
            if os.path.exists(font_path):
                os.remove(font_path)
            # ä»Šå›ã¯è‹±èªãƒ•ã‚©ãƒ³ãƒˆã§é€²ã‚ã‚‹ï¼ˆã‚¨ãƒ©ãƒ¼ã§æ­¢ã¾ã‚‹ã‚ˆã‚Šã¯ãƒã‚·ï¼‰
            pass

    styles = getSampleStyleSheet()
    
    # ã‚¹ã‚¿ã‚¤ãƒ«å®šç¾©
    style_body = ParagraphStyle(name='Body',
                                parent=styles['Normal'],
                                fontName=font_name,
                                fontSize=10.5,
                                leading=16,
                                spaceAfter=6,
                                alignment=TA_JUSTIFY)
                                
    style_title = ParagraphStyle(name='Title',
                                 parent=styles['Heading1'],
                                 fontName=font_name,
                                 fontSize=16,
                                 leading=20,
                                 spaceAfter=20)

    story = []
    
    # PDFç”Ÿæˆ
    story.append(Paragraph(title, style_title))
    
    for text in clean_text_list:
        if text.strip():
            # ç‰¹æ®Šæ–‡å­—ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
            safe_text = text.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')
            story.append(Paragraph(safe_text, style_body))
            story.append(Spacer(1, 2*mm))

    doc.build(story)
    buffer.seek(0)
    return buffer

# ==========================================
# ãƒ–ãƒ©ã‚¦ã‚¶æ“ä½œ
# ==========================================
def fetch_html_force_clean(target_url):
    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=True,
            args=['--no-sandbox', '--disable-dev-shm-usage', '--disable-gpu']
        )
        iphone_12 = p.devices['iPhone 12']
        context = browser.new_context(**iphone_12)
        page = context.new_page()

        try:
            page.goto(FIXED_ENTRY_URL, timeout=30000)
            time.sleep(2) 
            page.goto(target_url, timeout=30000)
            page.wait_for_load_state("domcontentloaded")
            time.sleep(2) 

            # ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ç ´å£Š
            page.evaluate("""
                () => {
                    const keywords = ['ã¯ã„', 'YES', 'Yes', '18æ­³', 'Enter', 'å…¥ã‚Šå£', 'å…¥å ´'];
                    const buttons = document.querySelectorAll('a, button, div, span');
                    for (let btn of buttons) {
                        if (keywords.some(k => btn.innerText && btn.innerText.includes(k))) {
                            btn.click();
                        }
                    }
                    const allDivs = document.querySelectorAll('body > div, body > section');
                    allDivs.forEach(div => {
                        const style = window.getComputedStyle(div);
                        if (style.position === 'fixed' && style.zIndex > 50) {
                            div.remove();
                        }
                    });
                    document.body.style.overflow = 'visible';
                    document.body.style.height = 'auto';
                }
            """)
            time.sleep(1) 
            return page.content()
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
            return None
        finally:
            browser.close()

# ==========================================
# æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯
# ==========================================
def extract_target_content(html_content, target_url):
    soup = BeautifulSoup(html_content, 'html.parser')

    styles = []
    for link in soup.find_all('link', rel='stylesheet'):
        styles.append(str(link))
    for style in soup.find_all('style'):
        styles.append(str(style))
    style_html = "\n".join(styles)

    # ã‚¿ã‚¤ãƒˆãƒ«
    title_html = ""
    title_text_clean = "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"
    target_h1 = soup.find("h1", class_="pageTitle")
    
    if target_h1:
        title_html = str(target_h1)
        title_text_clean = target_h1.get_text(strip=True)
    else:
        target_h1 = soup.find("h1")
        if target_h1:
            title_html = str(target_h1)
            title_text_clean = target_h1.get_text(strip=True)

    simple_title_text = soup.title.get_text(strip=True) if soup.title else "æŠ½å‡ºçµæœ"

    # æœ¬æ–‡
    body_html = "<div>æœ¬æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ</div>"
    text_list_for_file = []
    
    target_div = soup.find(id="sentenceBox")
    if not target_div:
        target_div = soup.find(id="main_txt")

    if target_div:
        for bad in target_div.find_all(["script", "noscript", "iframe", "form", "button", "input"]):
            bad.decompose()

        cut_point = target_div.find(class_="kakomiPop2")
        if cut_point:
            for sibling in cut_point.find_next_siblings():
                sibling.decompose()
            cut_point.decompose()

        body_html = str(target_div)
        
        for elem in target_div.find_all(['p', 'div', 'h2', 'h3', 'br']):
            txt = elem.get_text(strip=True)
            if txt:
                text_list_for_file.append(txt)

    final_html = f"""
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="utf-8">
        <base href="{target_url}">
        {style_html}
        <style>
            body {{
                background-color: #fff;
                padding: 15px;
                font-family: sans-serif;
                overflow: auto !important;
            }}
            h1.pageTitle {{
                font-size: 20px;
                margin-bottom: 20px;
                border-bottom: 1px solid #ccc;
                padding-bottom: 10px;
                line-height: 1.4;
            }}
            #sentenceBox {{
                font-size: 16px;
                line-height: 1.8;
                color: #333;
            }}
        </style>
    </head>
    <body>
        {title_html}
        {body_html}
    </body>
    </html>
    """

    return simple_title_text, title_text_clean, text_list_for_file, final_html

# ==========================================
# ç”»é¢æ§‹æˆ
# ==========================================
st.set_page_config(page_title="H-Review Pro", layout="wide")

st.title("ğŸ’ å®Œå…¨ç‰ˆãƒªãƒ¼ãƒ€ãƒ¼ (ä¿®å¾©æ©Ÿèƒ½ä»˜ã)")
st.caption("æŠ½å‡ºãƒ»è¡¨ç¤ºãƒ»Word/PDFä¿å­˜ãŒå¯èƒ½ã§ã™ã€‚")

col1, col2 = st.columns([3, 1])
with col1:
    url = st.text_input("èª­ã¿ãŸã„è¨˜äº‹ã®URL", placeholder="https://...")

if st.button("æŠ½å‡ºã™ã‚‹", type="primary"):
    if not url:
        st.warning("URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        status = st.empty()
        status.text("èª­ã¿è¾¼ã¿ä¸­...")
        
        html = fetch_html_force_clean(url)

        if html:
            status.text("ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä¸­...")
            
            page_title, article_title, text_list, final_html = extract_target_content(html, url)
            
            status.empty()
            st.success("å®Œäº†")
            
            st.sidebar.markdown("### ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
            
            # Word
            docx_file = create_docx(article_title, text_list)
            st.sidebar.download_button(
                label="ğŸ“„ Word (.docx) ã§ä¿å­˜",
                data=docx_file,
                file_name="story.docx",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
            )
            
            # PDF
            pdf_file = create_pdf(article_title, text_list)
            st.sidebar.download_button(
                label="ğŸ“• PDF (.pdf) ã§ä¿å­˜",
                data=pdf_file,
                file_name="story.pdf",
                mime="application/pdf"
            )

            components.html(final_html, height=800, scrolling=True)
            
        else:
            status.error("èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
