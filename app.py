```python
import streamlit as st
from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
import time
import subprocess

# --- è¨­å®šï¼šå…¥ã‚Šå£ã¨ãªã‚‹URL ---
FIXED_ENTRY_URL = "https://www.h-ken.net/mypage/20250611_1605697556/"

# --- ã‚µãƒ¼ãƒãƒ¼è¨­å®šï¼ˆåˆå›ã®ã¿å®Ÿè¡Œï¼‰ ---
def install_playwright():
    try:
        subprocess.run(["playwright", "install", "chromium"], check=True)
    except Exception as e:
        print(f"Install error: {e}")

if "setup_done" not in st.session_state:
    with st.spinner("ã‚µãƒ¼ãƒãƒ¼èµ·å‹•ä¸­...ï¼ˆåˆå›ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰"):
        install_playwright()
        st.session_state.setup_done = True

# --- ãƒ–ãƒ©ã‚¦ã‚¶æ“ä½œ ---
def fetch_html_via_route(target_url):
    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=True,
            args=['--no-sandbox', '--disable-dev-shm-usage', '--disable-gpu']
        )
        # iPhone 12è¨­å®š
        iphone_12 = p.devices['iPhone 12']
        context = browser.new_context(**iphone_12)
        page = context.new_page()

        try:
            # 1. å…¥ã‚Šå£URLã¸
            page.goto(FIXED_ENTRY_URL, timeout=30000)
            time.sleep(3) 

            # 2. ç›®çš„ã®URLã¸
            page.goto(target_url, timeout=30000)
            page.wait_for_load_state("networkidle")

            return page.content()

        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
            return None
        finally:
            browser.close()

# --- æŠ½å‡ºãƒ­ã‚¸ãƒƒã‚¯ï¼ˆè‰²ä»˜ãï¼‰ ---
def extract_colored_body(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')

    # ä¸è¦ãªã‚¿ã‚°å‰Šé™¤ï¼ˆè‰²ã¯æ®‹ã™ï¼‰
    for tag in soup(["script", "style", "nav", "footer", "header", "noscript", "iframe", "form", "button", "input", "meta", "link", "img", "svg"]):
        tag.decompose()

    # ã‚¿ã‚¤ãƒˆãƒ«
    title_text = "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"
    h1 = soup.find('h1')
    if h1:
        title_text = h1.get_text(strip=True)
    elif soup.title:
        title_text = soup.title.get_text(strip=True)

    # æœ¬æ–‡ï¼ˆHTMLä¿æŒï¼‰
    max_score = 0
    best_html = "<p>æœ¬æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ</p>"
    candidates = soup.find_all(['div', 'article', 'section', 'main'])

    for candidate in candidates:
        text = candidate.get_text(strip=True)
        score = len(text)
        
        # ãƒªãƒ³ã‚¯æ–‡å­—ç‡ãŒé«˜ã„ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å¤–
        links = candidate.find_all('a')
        link_len = sum([len(a.get_text()) for a in links])
        if score > 0 and (link_len / score) > 0.5:
            continue

        if score > max_score:
            max_score = score
            best_html = str(candidate)

    return title_text, best_html

# --- ç”»é¢æ§‹æˆ ---
st.set_page_config(page_title="Review Extractor", layout="centered")
st.title("ğŸ“± ä½“é¨“è«‡æŠ½å‡ºã‚¢ãƒ—ãƒª")
st.caption("æŒ‡å®šã®å…¥ã‚Šå£URLã‚’çµŒç”±ã—ã¦å†…å®¹ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")

url = st.text_input("èª­ã¿ãŸã„è¨˜äº‹ã®URL", placeholder="https://...")

if st.button("æŠ½å‡ºé–‹å§‹"):
    if not url:
        st.warning("URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    else:
        status = st.empty()
        status.text("èª­ã¿è¾¼ã¿ä¸­...")
        
        html = fetch_html_via_route(url)

        if html:
            title, body = extract_colored_body(html)
            status.empty()
            st.success("å®Œäº†")
            st.markdown(f"### {title}")
            st.divider()
            st.markdown(body, unsafe_allow_html=True)
            st.divider()
        else:
            status.error("å¤±æ•—ã—ã¾ã—ãŸã€‚")
```